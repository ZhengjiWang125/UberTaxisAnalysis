{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dab45e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edfc28a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ba3d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import logging\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy import Column, Integer, String\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy import text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbdc020",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd35f21",
   "metadata": {},
   "source": [
    "Running this part will download all the files in the local repo, but it will take a long time. \n",
    "Depending on the machine it runs on, it might throw error due to large size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25e908d",
   "metadata": {},
   "source": [
    "### Clean Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da62b602",
   "metadata": {},
   "source": [
    "All weather csv are cleaned and seperated into hours part and days part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60298343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the clean version of weather csv and seprated into two parts\n",
    "def create_weather_csv(filename):\n",
    "    # make sure the output title is consistent \n",
    "    title1 = \"\"\n",
    "    title2 = \"\"\n",
    "    m = re.search(r'(\\d+_weather)', filename)\n",
    "    if m:\n",
    "        title1 = m.group(1) + '_hours.csv'\n",
    "        title2 = m.group(1) + '_days.csv'\n",
    "    \n",
    "    \n",
    "    #this function cleans the weather csv and output 2 csv, one is for daily and one is for hour\n",
    "    df = pd.read_csv(filename,usecols=['DATE','HourlyWindSpeed','HourlyPrecipitation'])\n",
    "    #deal with missing value and special character\n",
    "    df = df.fillna(0)\n",
    "    df = df.replace('T',0)\n",
    "    df = df.replace('s','',regex = True)\n",
    "    #change the data type\n",
    "    df.DATE = pd.to_datetime(df.DATE)\n",
    "    df = df.astype({'HourlyPrecipitation':float,'HourlyWindSpeed':float})\n",
    "    df.set_index('DATE',drop = True).to_csv(title1)\n",
    "    df = df.resample('D', on='DATE').mean() \n",
    "    df = df.rename(mapper = {\"HourlyPrecipitation\":\"DailyPrecipitation\", \"HourlyWindSpeed\":\"DailyWindSpeed\"},axis = 1)\n",
    "    df.to_csv(title2)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a61e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2009,2016):\n",
    "    #create all weather data from 2009 to 2015\n",
    "    filename = str(i)+\"_weather.csv\"\n",
    "    create_weather_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e363d2",
   "metadata": {},
   "source": [
    "### Clean Uber Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f862a1",
   "metadata": {},
   "source": [
    "Clean uber sample data and makes the name consistent to the yellow taxi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabe5081",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber = pd.read_csv('uber_rides_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b72b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function gets the distance between two coordinates\n",
    "def get_distance(lon1,lat1,lon2,lat2):\n",
    "    from math import sin, cos, sqrt, atan2, radians\n",
    "    R = 6373\n",
    "    lon1 = radians(lon1)\n",
    "    lat1 = radians(lat1)\n",
    "    lon2 = radians(lon2)\n",
    "    lat2 = radians(lat2)\n",
    "    \n",
    "    dlon = lon1 - lon2\n",
    "    dlat = lat1 - lat2\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    \n",
    "    distance = R * c\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b835f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add distance to the dataframe with coordinates\n",
    "def add_distance(df):\n",
    "    distance = []\n",
    "    lon1 = list(df['pickup_longitude'])\n",
    "    lon2 = list(df['dropoff_longitude'])\n",
    "    lat1 = list(df['pickup_latitude'])\n",
    "    lat2 = list(df['dropoff_latitude'])\n",
    "    for i in range(len(lon1)):\n",
    "        distance.append(get_distance(lon1[i],lat1[i],lon2[i],lat2[i]))\n",
    "    df['distance']  = distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40562a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the clean version of uber dataframe\n",
    "def create_uber(df):\n",
    "    #clean uber data\n",
    "    df = df.rename(columns = lambda x: x.strip())\n",
    "   \n",
    "    #drop and rename column\n",
    "    to_drop = [\n",
    "        \"Unnamed: 0\",\n",
    "        \"key\",\n",
    "        \"passenger_count\"\n",
    "    ]\n",
    "    \n",
    "    mapper = {\n",
    "        \"pickup_datetime\" :\"pickup_time\",\n",
    "        \"fare_amount\" : \"charge\"\n",
    "    }\n",
    "    df = df.drop(to_drop, axis = 1,errors = \"ignore\")\n",
    "    df = df.rename(mapper, axis = 1)\n",
    "    df.dropna(inplace = True)\n",
    "    \n",
    "    #modify datatype\n",
    "    df = df.astype({\"pickup_time\":np.datetime64})\n",
    "    \n",
    "    \n",
    "    #add distance\n",
    "    add_distance(df)\n",
    "    df.to_csv('uber.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd75b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_uber(uber)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc180a4",
   "metadata": {},
   "source": [
    "### Clean Yellow Taxi Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390472d4",
   "metadata": {},
   "source": [
    "find all csv files and filter them, then download as a dataframe and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a692d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_links():\n",
    "    #this function visitsï¼š https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "    #It requests the source code on the website and get all the hrefs related to csv\n",
    "    #the urls are saved in link_lists\n",
    "    link_lists = []\n",
    "    url = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    aclasses = soup.find_all('a')\n",
    "    for a in aclasses:\n",
    "        link_lists.append(a.get('href'))\n",
    "    #Then we filter on link_lists using re because we only want to grab csv for yellow taxi ranging from 2009 - 2015.\n",
    "    csv_links = []\n",
    "    pattern = re.compile(r'.yellow_tripdata_(200[9]|201[0-5])-\\d\\d\\.csv$')\n",
    "    for i in link_lists:\n",
    "        if re.search(pattern,i):\n",
    "            csv_links.append(i)\n",
    "    return csv_links\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f111f166",
   "metadata": {},
   "outputs": [],
   "source": [
    "westlimit=-74.242330; southlimit=40.560445; eastlimit=-73.717047; northlimit=40.908524\n",
    "#Remove the data that is not within the limits specified above\n",
    "\n",
    "def fix_longitude(input_longitude):\n",
    "    try:\n",
    "        input_longitude = float(input_longitude)\n",
    "    except:\n",
    "        return np.NaN\n",
    "    if input_longitude < westlimit or input_longitude > eastlimit:\n",
    "        return np.NaN\n",
    "    return input_longitude\n",
    "\n",
    "\n",
    "def fix_latitude(input_latitude):\n",
    "    try:\n",
    "        input_latitude = float(input_latitude)\n",
    "    except:\n",
    "        return np.NaN\n",
    "    if input_latitude < southlimit or input_latitude > northlimit:\n",
    "        return np.NaN\n",
    "    return input_latitude\n",
    "\n",
    "\n",
    "def fix_df(df):\n",
    "    df['pickup_longitude']=df['pickup_longitude'].apply(fix_longitude)\n",
    "    df['dropoff_longitude']=df['dropoff_longitude'].apply(fix_longitude)\n",
    "    df['pickup_latitude']=df['pickup_latitude'].apply(fix_latitude)\n",
    "    df['dropoff_latitude']=df['dropoff_latitude'].apply(fix_latitude)\n",
    "    df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd606cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv(csv_link):\n",
    "   \n",
    "    # make sure the output title is consistent \n",
    "    title = \"\"\n",
    "    m = re.search(r'(yellow.+)', csv_link)\n",
    "    if m:\n",
    "        title = m.group(1)\n",
    "   \n",
    "    #read the data into a dataframe\n",
    "    df = pd.read_csv(csv_link,on_bad_lines='skip')\n",
    "    df = df.rename(columns = lambda x: x.strip())\n",
    "   \n",
    "    #drop and rename column\n",
    "    to_drop = [\n",
    "        \"Unnamed: 0\",\n",
    "        \"vendor_name\",\n",
    "        \"vendor_id\",\n",
    "        \"Vendor_id\",\n",
    "        'VendorID',\n",
    "        \"Trip_distance\",\n",
    "        \"Trip_Distance\",\n",
    "        \"trip_distance\",\n",
    "        \"Rate_Code\",\n",
    "        \"store_and_forward\",\n",
    "        \"store_and_fwd_flag\",\n",
    "        \"Payment_Type\",\n",
    "        \"Fare_Amt\",\n",
    "        \"surcharge\",\n",
    "        \"mta_tax\",\n",
    "        \"Tolls_Amt\",\n",
    "        \"rate_code\",\n",
    "        \"RatecodeID\",\n",
    "        \"RateCodeID\",\n",
    "        \"payment_type\",\n",
    "        \"fare_amount\",\n",
    "        \"extra\",\n",
    "        \"tolls_amount\",\n",
    "        \"improvement_surcharge\",\n",
    "        \"Passenger_Count\",\n",
    "        \"passenger_count\"\n",
    "    ]\n",
    "    \n",
    "    mapper = {\n",
    "        \"Trip_Pickup_DateTime\" : \"pickup_time\",\n",
    "        \"tpep_pickup_datetime\" : \"pickup_time\",\n",
    "        \"pickup_datetime\": \"pickup_time\",\n",
    "        \"dropoff_datetime\" : \"dropoff_time\",\n",
    "        \"Trip_Dropoff_DateTime\" : \"dropoff_time\",\n",
    "        \"tpep_dropoff_datetime\" : \"dropoff_time\",\n",
    "        \"Start_Lon\" : \"pickup_longitude\",\n",
    "        \"Start_Lat\" : \"pickup_latitude\",\n",
    "        \"End_Lon\" : \"dropoff_longitude\",\n",
    "        \"End_Lat\" : \"dropoff_latitude\",\n",
    "        \"Tip_Amt\" : 'tip',\n",
    "        \"tip_amount\" : \"tip\",\n",
    "        \"Total_Amt\" : \"charge\",\n",
    "        \"total_amount\" : \"charge\"\n",
    "    }\n",
    "    df = df.drop(to_drop, axis = 1,errors = \"ignore\")\n",
    "    df = df.rename(mapper, axis = 1)\n",
    "    df.dropna(inplace = True)\n",
    "    \n",
    "    #modify datatype\n",
    "    df = df.astype({\"pickup_time\":np.datetime64,\"dropoff_time\": np.datetime64})\n",
    "    \n",
    "    \n",
    "    #make sure the trip is within(40.560445, -74.242330) and (40.908524, -73.717047)\n",
    "    fix_df(df)\n",
    "    \n",
    "    #sample 3000 rows\n",
    "    df = df.sample(n=3000)\n",
    "    \n",
    "    #add distance\n",
    "    add_distance(df)\n",
    "    df.reset_index(inplace = True, drop = True)\n",
    "    \n",
    "    df.to_csv(title,index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5684a7d8",
   "metadata": {},
   "source": [
    "run this to download all the yellow taxi sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd07a621",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "links = get_csv_links()\n",
    "for i in tqdm(range(len(links))):\n",
    "    create_csv(links[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f936ede",
   "metadata": {},
   "source": [
    "# Storing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d23713",
   "metadata": {},
   "source": [
    "We first read all csv files into 4 dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97da7cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createuber():\n",
    "    #this function reads in uber.csv and returns the data in pd.DataFrame format\n",
    "    uber = pd.read_csv('uber.csv')\n",
    "    uber.pickup_time = pd.to_datetime(uber.pickup_time)\n",
    "    uber = uber[uber.distance != 0]\n",
    "    return uber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b3224b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createtaxi():\n",
    "    #this function reads in yellow_trip_{year}-{month}.csv and returns the data in pd.DataFrame format\n",
    "    filenames = []\n",
    "    for i in range(2009,2016):\n",
    "        for j in range(1,13):\n",
    "            if j <10:\n",
    "                filename = f'yellow_tripdata_{i}-0{j}.csv'\n",
    "                filenames.append(filename)\n",
    "            else:\n",
    "                filename = f'yellow_tripdata_{i}-{j}.csv'\n",
    "                filenames.append(filename)\n",
    "    taxi = pd.read_csv('yellow_tripdata_2009-01.csv',usecols = ['pickup_time','dropoff_time','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','charge','tip','distance'],nrows = 2314)\n",
    "    for i in range(1,len(filenames)):\n",
    "        df = pd.read_csv(filenames[i],usecols = ['pickup_time','dropoff_time','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','charge','tip','distance'], nrows = 2314)\n",
    "        taxi = pd.concat([taxi,df])\n",
    "    taxi.reset_index(inplace = True, drop = True)\n",
    "    taxi.pickup_time = pd.to_datetime(taxi.pickup_time)\n",
    "    taxi.dropoff_time = pd.to_datetime(taxi.dropoff_time)\n",
    "    return taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "304291f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createhourlyweather():\n",
    "    #this function reads in {year}_weather_hours.csv and returns the data in pd.DataFrame format\n",
    "    filenames = []\n",
    "    for i in range(2009,2016):\n",
    "        filenames.append(f'{i}_weather_hours.csv')\n",
    "    weather_hourly = pd.read_csv('2009_weather_hours.csv')\n",
    "    for i in range(1,len(filenames)):\n",
    "        df = pd.read_csv(filenames[i])\n",
    "        weather_hourly = pd.concat([weather_hourly,df])\n",
    "    weather_hourly.reset_index(inplace = True, drop = True)\n",
    "    weather_hourly.DATE = pd.to_datetime(weather_hourly.DATE)\n",
    "    weather_hourly['DATE'] = weather_hourly['DATE'].dt.floor('H')\n",
    "    weather_hourly = weather_hourly.drop_duplicates('DATE',keep = 'first')\n",
    "    mapper = {'DATE': 'date', \"HourlyPrecipitation\":'precipitation','HourlyWindSpeed':'windspeed'}\n",
    "    weather_hourly = weather_hourly.rename(mapper,axis = 1)\n",
    "    return weather_hourly\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43347ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createdailyweather():\n",
    "    #this function reads in {year}_weather_days.csv and returns the data in pd.DataFrame format\n",
    "    filenames = []\n",
    "    for i in range(2009,2016):\n",
    "        filenames.append(f'{i}_weather_days.csv')\n",
    "    weather_daily = pd.read_csv('2009_weather_days.csv')\n",
    "    for i in range(1,len(filenames)):\n",
    "        df = pd.read_csv(filenames[i])\n",
    "        weather_daily = pd.concat([weather_daily,df])\n",
    "    weather_daily.reset_index(inplace = True, drop = True)\n",
    "    weather_daily.DATE = pd.to_datetime(weather_daily.DATE)\n",
    "    mapper = {'DATE': 'date', \"DailyPrecipitation\":'precipitation','DailyWindSpeed':'windspeed'}\n",
    "    weather_daily = weather_daily.rename(mapper,axis = 1)\n",
    "    return weather_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a17c6c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber = createuber()\n",
    "taxi = createtaxi()\n",
    "weather_hourly= createhourlyweather()\n",
    "weather_daily = createdailyweather()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b863fbcb",
   "metadata": {},
   "source": [
    "Now we try to build database and four tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f66c9d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sqlalchemy] BEGIN (implicit)\n",
      "[sqlalchemy] COMMIT\n"
     ]
    }
   ],
   "source": [
    "#build database\n",
    "engine = create_engine(f\"sqlite:///ubertaxi.db\", echo=True)\n",
    "sqllogger = logging.getLogger(\"sqlalchemy.engine.Engine\")\n",
    "formatter = logging.Formatter(\"[sqlalchemy] %(message)s\")\n",
    "sqllogger.handlers[0].setFormatter(formatter)\n",
    "Base = declarative_base()\n",
    "Base.metadata.create_all(engine, checkfirst=True)\n",
    "# sessionmaker returns a Session class\n",
    "Session = sessionmaker(bind=engine)\n",
    "# and we create an instance of Session\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2dd0298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readsqlfile(filename):\n",
    "    #I am gonna be honest, I found this on stack overflow. Basically it allows the sqlalchemy to execute multiple queries \n",
    "    #in one file. \n",
    "    sql_file = open('schema.sql','r')\n",
    "\n",
    "    # Create an empty command string\n",
    "    sql_command = ''\n",
    "    for line in sql_file:\n",
    "        # Ignore commented lines\n",
    "        if not line.startswith('--') and line.strip('\\n'):\n",
    "            # Append line to the command string\n",
    "            sql_command += line.strip('\\n')\n",
    "            # If the command string ends with ';', it is a full statement\n",
    "            if sql_command.endswith(';'):\n",
    "                # Try to execute statement and commit it\n",
    "                try:\n",
    "                    session.execute(text(sql_command))\n",
    "                    session.commit()\n",
    "                # Assert in case of error\n",
    "                except:\n",
    "                    print('Ops')\n",
    "                # Finally, clear command string\n",
    "                finally:\n",
    "                    sql_command = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced06338",
   "metadata": {},
   "source": [
    "Read schema file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "320edc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ops\n",
      "Ops\n",
      "Ops\n",
      "Ops\n"
     ]
    }
   ],
   "source": [
    "readsqlfile('schema.sql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf37e5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sqlalchemy] PRAGMA main.table_info(\"Uber\")\n",
      "[sqlalchemy] [raw sql] ()\n",
      "[sqlalchemy] PRAGMA temp.table_info(\"Uber\")\n",
      "[sqlalchemy] [raw sql] ()\n",
      "[sqlalchemy] BEGIN (implicit)\n",
      "[sqlalchemy] \n",
      "CREATE TABLE \"Uber\" (\n",
      "\tcharge FLOAT, \n",
      "\tpickup_time DATETIME, \n",
      "\tpickup_longitude FLOAT, \n",
      "\tpickup_latitude FLOAT, \n",
      "\tdropoff_longitude FLOAT, \n",
      "\tdropoff_latitude FLOAT, \n",
      "\tdistance FLOAT\n",
      ")\n",
      "\n",
      "\n",
      "[sqlalchemy] [no key 0.00068s] ()\n",
      "[sqlalchemy] COMMIT\n",
      "[sqlalchemy] BEGIN (implicit)\n",
      "[sqlalchemy] INSERT INTO \"Uber\" (charge, pickup_time, pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude, distance) VALUES (?, ?, ?, ?, ?, ?, ?)\n",
      "[sqlalchemy] [generated in 1.76549s] ((7.5, '2015-05-07 19:52:06.000000', -73.99981689453125, 40.73835372924805, -73.99951171875, 40.72321701049805, 1.6838511852242786), (7.7, '2009-07-17 20:04:56.000000', -73.994355, 40.728225, -73.99471, 40.750325, 2.458361376443877), (12.9, '2009-08-24 21:45:00.000000', -74.005043, 40.74077, -73.962565, 40.772647, 5.0379582221658445), (5.3, '2009-06-26 08:22:21.000000', -73.976124, 40.790844, -73.965316, 40.803349, 1.6622050981962735), (16.0, '2014-08-28 17:47:00.000000', -73.925023, 40.744085, -73.97308199999999, 40.761247, 4.4768549072953325), (24.5, '2014-10-12 07:04:00.000000', -73.96144699999999, 40.693965000000006, -73.871195, 40.774297, 11.734697512602486), (9.7, '2012-02-17 09:32:00.000000', -73.975187, 40.745767, -74.00272, 40.743537, 2.333443299721285), (12.5, '2012-03-29 19:06:00.000000', -74.001065, 40.741787, -73.96304, 40.775012, 4.890951785888044)  ... displaying 10 of 194367 total bound parameter sets ...  (14.5, '2015-05-20 14:56:25.000000', -73.99712371826173, 40.7254524230957, -73.98321533203125, 40.69541549682617, 3.5408266479309387), (14.1, '2010-05-15 04:08:00.000000', -73.98439499999999, 40.720077, -73.985508, 40.768793, 5.419484244981255))\n",
      "[sqlalchemy] COMMIT\n",
      "[sqlalchemy] SELECT name FROM sqlite_master WHERE type='table' ORDER BY name\n",
      "[sqlalchemy] [raw sql] ()\n",
      "[sqlalchemy] PRAGMA main.table_info(\"yellow_taxi\")\n",
      "[sqlalchemy] [raw sql] ()\n",
      "[sqlalchemy] PRAGMA temp.table_info(\"yellow_taxi\")\n",
      "[sqlalchemy] [raw sql] ()\n",
      "[sqlalchemy] BEGIN (implicit)\n",
      "[sqlalchemy] \n",
      "CREATE TABLE yellow_taxi (\n",
      "\tpickup_time DATETIME, \n",
      "\tdropoff_time DATETIME, \n",
      "\tpickup_longitude FLOAT, \n",
      "\tpickup_latitude FLOAT, \n",
      "\tdropoff_longitude FLOAT, \n",
      "\tdropoff_latitude FLOAT, \n",
      "\ttip FLOAT, \n",
      "\tcharge FLOAT, \n",
      "\tdistance FLOAT\n",
      ")\n",
      "\n",
      "\n",
      "[sqlalchemy] [no key 0.00061s] ()\n",
      "[sqlalchemy] COMMIT\n",
      "[sqlalchemy] BEGIN (implicit)\n",
      "[sqlalchemy] INSERT INTO yellow_taxi (pickup_time, dropoff_time, pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude, tip, charge, distance) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
      "[sqlalchemy] [generated in 2.44100s] (('2009-01-10 22:00:53.000000', '2009-01-10 22:12:59.000000', -73.985455, 40.735604, -73.980652, 40.763299, 0.5999999999999999, 10.0, 3.106983034371679), ('2009-01-20 17:50:03.000000', '2009-01-20 18:11:48.000000', -73.946053, 40.775474, -73.982416, 40.764801, 0.0, 13.5, 3.285159434004516), ('2009-01-05 11:27:15.000000', '2009-01-05 11:40:23.000000', -73.96777199999998, 40.786988, -73.95498, 40.76623, 0.0, 9.3, 2.547941290711391), ('2009-01-03 12:01:37.000000', '2009-01-03 12:05:31.000000', -73.979783, 40.7572, -73.973967, 40.766057, 0.0, 4.5, 1.1002900803175624), ('2009-01-18 01:01:06.000000', '2009-01-18 01:06:11.000000', -73.973095, 40.744304, -73.95614899999998, 40.771973, 0.0, 7.4, 3.3926733836607244), ('2009-01-06 00:07:16.000000', '2009-01-06 00:23:58.000000', -73.874791, 40.774081, -73.953866, 40.818769, 0.0, 23.15, 8.30920976716833), ('2009-01-17 14:38:00.000000', '2009-01-17 14:51:00.000000', -74.003163, 40.731217, -74.01419199999998, 40.717087, 0.0, 8.5, 1.8260682409046776), ('2009-01-18 02:22:00.000000', '2009-01-18 02:26:00.000000', -74.000253, 40.737798, -74.000253, 40.737798, 0.0, 4.6, 0.0)  ... displaying 10 of 194376 total bound parameter sets ...  ('2015-12-16 16:54:54.000000', '2015-12-16 16:57:34.000000', -73.94960021972656, 40.77705001831056, -73.95596313476561, 40.77965927124024, 0.4, 6.2, 0.609473353061974), ('2015-12-23 22:34:02.000000', '2015-12-23 22:39:05.000000', -73.97403717041014, 40.762657165527344, -73.97759246826173, 40.7539176940918, 0.0, 6.3, 1.0171953963618967))\n",
      "[sqlalchemy] COMMIT\n",
      "[sqlalchemy] PRAGMA main.table_info(\"weather_hourly\")\n",
      "[sqlalchemy] [raw sql] ()\n",
      "[sqlalchemy] PRAGMA temp.table_info(\"weather_hourly\")\n",
      "[sqlalchemy] [raw sql] ()\n",
      "[sqlalchemy] BEGIN (implicit)\n",
      "[sqlalchemy] \n",
      "CREATE TABLE weather_hourly (\n",
      "\tdate DATETIME, \n",
      "\tprecipitation FLOAT, \n",
      "\twindspeed FLOAT\n",
      ")\n",
      "\n",
      "\n",
      "[sqlalchemy] [no key 0.00052s] ()\n",
      "[sqlalchemy] COMMIT\n",
      "[sqlalchemy] BEGIN (implicit)\n",
      "[sqlalchemy] INSERT INTO weather_hourly (date, precipitation, windspeed) VALUES (?, ?, ?)\n",
      "[sqlalchemy] [generated in 0.34296s] (('2009-01-01 00:00:00.000000', 0.0, 18.0), ('2009-01-01 01:00:00.000000', 0.0, 18.0), ('2009-01-01 02:00:00.000000', 0.0, 18.0), ('2009-01-01 03:00:00.000000', 0.0, 8.0), ('2009-01-01 04:00:00.000000', 0.0, 11.0), ('2009-01-01 05:00:00.000000', 0.0, 18.0), ('2009-01-01 06:00:00.000000', 0.0, 14.0), ('2009-01-01 07:00:00.000000', 0.0, 8.0)  ... displaying 10 of 60458 total bound parameter sets ...  ('2015-12-31 22:00:00.000000', 0.0, 7.0), ('2015-12-31 23:00:00.000000', 0.0, 5.0))\n",
      "[sqlalchemy] COMMIT\n",
      "[sqlalchemy] PRAGMA main.table_info(\"weather_daily\")\n",
      "[sqlalchemy] [raw sql] ()\n",
      "[sqlalchemy] PRAGMA temp.table_info(\"weather_daily\")\n",
      "[sqlalchemy] [raw sql] ()\n",
      "[sqlalchemy] BEGIN (implicit)\n",
      "[sqlalchemy] \n",
      "CREATE TABLE weather_daily (\n",
      "\tdate DATETIME, \n",
      "\tprecipitation FLOAT, \n",
      "\twindspeed FLOAT\n",
      ")\n",
      "\n",
      "\n",
      "[sqlalchemy] [no key 0.00090s] ()\n",
      "[sqlalchemy] COMMIT\n",
      "[sqlalchemy] BEGIN (implicit)\n",
      "[sqlalchemy] INSERT INTO weather_daily (date, precipitation, windspeed) VALUES (?, ?, ?)\n",
      "[sqlalchemy] [generated in 0.01494s] (('2009-01-01 00:00:00.000000', 0.0, 11.041666666666666), ('2009-01-02 00:00:00.000000', 0.0, 6.59375), ('2009-01-03 00:00:00.000000', 0.0, 9.875), ('2009-01-04 00:00:00.000000', 0.0, 7.37037037037037), ('2009-01-05 00:00:00.000000', 0.0, 6.925925925925926), ('2009-01-06 00:00:00.000000', 0.004, 6.9), ('2009-01-07 00:00:00.000000', 0.0466666666666666, 9.58974358974359), ('2009-01-08 00:00:00.000000', 0.0, 11.192307692307692)  ... displaying 10 of 2556 total bound parameter sets ...  ('2015-12-30 00:00:00.000000', 0.0074358974358974, 4.076923076923077), ('2015-12-31 00:00:00.000000', 0.0022857142857142, 4.2))\n",
      "[sqlalchemy] COMMIT\n"
     ]
    }
   ],
   "source": [
    "#add four dataframes into sql tables\n",
    "#DONT EXECUTE TWINCE!!!!!\n",
    "'''\n",
    "uber.to_sql('Uber', engine, if_exists='append',index = False)\n",
    "taxi.to_sql('yellow_taxi', engine, if_exists='append',index = False)\n",
    "weather_hourly.to_sql('weather_hourly', engine, if_exists='append',index = False)\n",
    "weather_daily.to_sql('weather_daily', engine, if_exists='append',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71487e0f",
   "metadata": {},
   "source": [
    "# Understanding Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7e4d08",
   "metadata": {},
   "source": [
    "Simply for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e53b89de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sqlalchemy] SELECT COUNT(*) FROM yellow_taxi\n",
      "[sqlalchemy] [raw sql] ()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(194376,)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(\"SELECT COUNT(*) FROM yellow_taxi\").fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d17f4950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifically for the file that only contains one command\n",
    "def readsql(filename):\n",
    "    with engine.connect() as con:\n",
    "        with open(filename) as file:\n",
    "            query = text(file.read())\n",
    "            return engine.execute(query).fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9799c7d",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "51a0380c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sqlalchemy] SELECT \n",
      "strftime ('%H',pickup_time) as most_popular_hour,\n",
      "Count(*) as times\n",
      "FROM\n",
      "yellow_taxi\n",
      "WHERE\n",
      "pickup_time between '2009-01-01' AND '2015-06-30'\n",
      "GROUP BY strftime ('%H',pickup_time) \n",
      "ORDER BY times DESC\n",
      "LIMIT 1;\n",
      "[sqlalchemy] [generated in 0.00084s] ()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('19', 11260)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readsql('most_popular_hour.sql')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86338029",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4fc48f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sqlalchemy] SELECT \n",
      "strftime ('%w',pickup_time) as most_popular_day,\n",
      "Count(*) as times\n",
      "FROM\n",
      "Uber\n",
      "WHERE\n",
      "pickup_time between '2009-01-01' AND '2015-06-30'\n",
      "GROUP BY strftime ('%w',pickup_time) \n",
      "ORDER BY times DESC\n",
      "LIMIT 1;\n",
      "[sqlalchemy] [generated in 0.00136s] ()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('5', 29996)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readsql('most_popular_day.sql')\n",
    "#So it is Friday! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6b8701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
